{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN38I1cHHwCXhTnp3KhQDgx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaju4568/shajahan/blob/main/Amazon_scrap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1lc0EjesY_nA"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Function to scrape product data from a single URL\n",
        "def scrape_product_data(url):\n",
        "    try:\n",
        "        # Send an HTTP GET request to the URL\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Check for any HTTP errors\n",
        "\n",
        "        # Parse the HTML content of the page\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Extract product information\n",
        "        product_title = soup.find('span', {'id': 'productTitle'}).text.strip()\n",
        "        product_image_url = soup.find('img', {'id': 'landingImage'})['src']\n",
        "        product_price = soup.find('span', {'id': 'priceblock_ourprice'}).text.strip()\n",
        "        product_details = soup.find('div', {'id': 'productDescription'}).text.strip()\n",
        "\n",
        "        return {\n",
        "            \"Product Title\": product_title,\n",
        "            \"Product Image URL\": product_image_url,\n",
        "            \"Price of the Product\": product_price,\n",
        "            \"Product Details\": product_details\n",
        "        }\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to scrape data from a batch of URLs\n",
        "def scrape_batch(urls):\n",
        "    results = []\n",
        "\n",
        "    for url in urls:\n",
        "        data = scrape_product_data(url)\n",
        "\n",
        "        if data:\n",
        "            results.append(data)\n",
        "        else:\n",
        "            print(f\"URL not available: {url}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Load URLs from the provided CSV file\n",
        "    csv_url = \"https://docs.google.com/spreadsheets/d/1BZSPhk1LDrx8ytywMHWVpCqbm8URTxE4mTJrIRkD7PnGTM/export?format=csv\"\n",
        "    response = requests.get(csv_url)\n",
        "    csv_data = response.text.splitlines()\n",
        "\n",
        "    # Remove the header row\n",
        "    csv_data.pop(0)\n",
        "\n",
        "    # Split the CSV data into batches of 100 URLs each\n",
        "    batch_size = 100\n",
        "    url_batches = [csv_data[i:i+batch_size] for i in range(0, len(csv_data), batch_size)]\n",
        "\n",
        "    # Scrape data from each batch\n",
        "    total_results = []\n",
        "    for i, url_batch in enumerate(url_batches):\n",
        "        print(f\"Scraping batch {i + 1}...\")\n",
        "        batch_results = scrape_batch(url_batch)\n",
        "        total_results.extend(batch_results)\n",
        "\n",
        "        # Sleep for a few seconds between batches to avoid overloading the server\n",
        "        time.sleep(5)\n",
        "\n",
        "    # Save the results to a JSON file\n",
        "    with open('amazon_product_data.json', 'w', encoding='utf-8') as json_file:\n",
        "        json.dump(total_results, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}